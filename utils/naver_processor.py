import io
import json
import re
from typing import Any, Callable

import pandas as pd


def clean_columns(df: pd.DataFrame) -> pd.DataFrame:
    """Strip whitespace from column names."""
    return df.rename(columns=lambda c: str(c).strip())


def _normalize_order(value):
    """Normalize order number for matching - remove all spaces and convert to string."""
    if pd.isna(value):
        return ""
    if isinstance(value, float):
        return str(int(value))
    if isinstance(value, int):
        return str(value)
    s = str(value).strip()
    # Remove all whitespace characters (spaces, tabs, newlines, etc.)
    s = "".join(s.split())
    if s.endswith(".0") and s.replace(".", "", 1).isdigit():
        try:
            return str(int(float(s)))
        except Exception:
            return s
    return s


def parse_naver_option(option_str: str) -> dict:
    """Parse ì˜µì…˜ì •ë³´ field into structured values."""
    result = {
        "ë³´ë‚´ì‹œëŠ”ë¶„": "",
        "ë„ì°©í¬ë§ë‚ ì§œ_ì›ë³¸": "",
        "ê³¼ì¼ì„ ë¬¼ì˜µì…˜": "",
        "í¬ë¦¬ìŠ¤íƒˆë³´ìê¸°": "",
    }

    if pd.isna(option_str):
        return result

    parts = str(option_str).split(" / ")

    for part in parts:
        if ":" in part:
            key, value = part.split(":", 1)
            key = key.strip()
            value = value.strip()

            if "ë³´ë‚´ì‹œëŠ” ë¶„" in key:
                result["ë³´ë‚´ì‹œëŠ”ë¶„"] = value
            elif "ë„ì°© í¬ë§ ë‚ ì§œ" in key or "ë„ì°©í¬ë§ë‚ ì§œ" in key:
                result["ë„ì°©í¬ë§ë‚ ì§œ_ì›ë³¸"] = value
            elif "ê³¼ì¼ ì„ ë¬¼ ì˜µì…˜" in key or "ê³¼ì¼ì„ ë¬¼ì˜µì…˜" in key:
                result["ê³¼ì¼ì„ ë¬¼ì˜µì…˜"] = value
            elif "í¬ë¦¬ìŠ¤íƒˆ ë³´ìê¸°" in key:
                result["í¬ë¦¬ìŠ¤íƒˆë³´ìê¸°"] = value

    return result


def normalize_dates_batch_with_ai(api_key: str, date_list: list) -> dict:
    """Use OpenAI Responses API to normalize a batch of date strings."""
    try:
        from openai import OpenAI

        client = OpenAI(api_key=api_key)

        dates_json = json.dumps(date_list, ensure_ascii=False)

        prompt = f"""
ë‹¤ìŒ JSON ë°°ì—´ì˜ ê° ë‚ ì§œ í…ìŠ¤íŠ¸ë¥¼ YYYY-MM-DD í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•´ì£¼ì„¸ìš”.
ë‚ ì§œ ì •ë³´ê°€ ë¶ˆí™•ì‹¤í•˜ë‹¤ê³  íŒë‹¨ë ë•ŒëŠ” ì¸í’‹ì— ìˆëŠ” ë‚ ì§œ ì •ë³´ë¥¼ ì°¸ê³ í•´ì„œ ë³€í™˜í•˜ì„¸ìš”. 
ë³€í™˜í•˜ëŠ” ë‚ ì§œë“¤ì€ ë¹„ìŠ·í•œ ì‹œì ì…ë‹ˆë‹¤.
10ì›” 1ì¼ ë˜ëŠ” 2ì¼ ì´ëŸ° ë‚ ì§œëŠ” 10ì›” 1ì¼ë¡œ ë³€í™˜í•˜ì„¸ìš”.
9ì›” 30ì¼ ë˜ëŠ” 10ì›” 1ì¼ ì´ëŸ° ë‚ ì§œëŠ” 9ì›” 30ì¼ë¡œ ë³€í™˜í•˜ì„¸ìš”.
10ì›” 2ì¼ ëŠ” 2025-10-02 ì´ëŸ°ì‹ìœ¼ë¡œ
ëª…í™•í•œ ë‚ ì§œê°€ ì•„ë‹Œê²½ìš°ëŠ” ë³€í™˜í•˜ì§€ ë§ê³  ê·¸ëŒ€ë¡œ ë‹¤ì‹œ ë³€í™˜ê²°ê³¼ì— ë„£ì–´ì¤˜.

ì…ë ¥: {dates_json}

ì¶œë ¥ì€ ë°˜ë“œì‹œ "ì›ë³¸": "ë³€í™˜ê²°ê³¼" í˜•íƒœì˜ JSON ê°ì²´ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”. ì„¤ëª…ì€ í•˜ì§€ ë§ˆì„¸ìš”.
ì˜ˆì‹œ: {{"9ì›”30ì¼": "2025-09-30", "10/1": "2025-10-01", "ìµœëŒ€í•œ ë¹¨ë¦¬": "ë¹ ë¥¸ë°°ì†¡"}}
"""

        response = client.responses.create(
            model="gpt-4.1-nano-2025-04-14",
            input=prompt,
            max_output_tokens=1000,
        )

        result_text = (response.output_text or "").strip()

        if not result_text:
            return {date: "ì˜¤ë¥˜: ë¹ˆ ì‘ë‹µ" for date in date_list}

        if result_text.startswith("```"):
            parts = result_text.split("```")
            if len(parts) >= 2:
                result_text = parts[1]
            result_text = result_text.replace("json", "").strip()

        json_match = re.search(r"\{.*\}", result_text, re.DOTALL)
        if json_match:
            result_text = json_match.group(0).strip()
        else:
            return {date: "ì˜¤ë¥˜: JSON ì¶œë ¥ ì•„ë‹˜" for date in date_list}

        try:
            return json.loads(result_text)
        except Exception:
            return {date: f"ì˜¤ë¥˜: JSON íŒŒì‹± ì‹¤íŒ¨: {result_text}" for date in date_list}

    except Exception as e:
        return {date: f"ì˜¤ë¥˜: {str(e)}" for date in date_list}


def create_naver_intermediate_table(df: pd.DataFrame, api_key: str | None = None) -> pd.DataFrame:
    """Build intermediate table from raw Naver export."""
    parsed_options = df["ì˜µì…˜ì •ë³´"].apply(parse_naver_option)
    parsed_df = pd.DataFrame(parsed_options.tolist())

    intermediate = pd.DataFrame(
        {
            "ìƒí’ˆì£¼ë¬¸ë²ˆí˜¸": df["ìƒí’ˆì£¼ë¬¸ë²ˆí˜¸"],
            "ìˆ˜ì·¨ì¸ëª…": df["ìˆ˜ì·¨ì¸ëª…"],
            "ìˆ˜ì·¨ì¸ì—°ë½ì²˜1": df["ìˆ˜ì·¨ì¸ì—°ë½ì²˜1"],
            "í†µí•©ë°°ì†¡ì§€": df["í†µí•©ë°°ì†¡ì§€"],
            "ë°°ì†¡ë©”ì„¸ì§€": df["ë°°ì†¡ë©”ì„¸ì§€"],
            "ìˆ˜ëŸ‰": df["ìˆ˜ëŸ‰"],
            "ì˜µì…˜ê´€ë¦¬ì½”ë“œ": df["ì˜µì…˜ê´€ë¦¬ì½”ë“œ"],
            "ë³´ë‚´ì‹œëŠ”ë¶„": parsed_df["ë³´ë‚´ì‹œëŠ”ë¶„"],
            "ë„ì°©í¬ë§ë‚ ì§œ_ì›ë³¸": parsed_df["ë„ì°©í¬ë§ë‚ ì§œ_ì›ë³¸"],
            "ë„ì°©í¬ë§ë‚ ì§œ_ì •ê·œí™”": "",
            "ê³¼ì¼ì„ ë¬¼ì˜µì…˜": parsed_df["ê³¼ì¼ì„ ë¬¼ì˜µì…˜"],
        }
    )

    return intermediate


def normalize_dates_batch(
    intermediate_df: pd.DataFrame,
    api_key: str,
    progress_callback: Callable[[int, int], Any] | None = None,
    debug_callback: Callable[[str, Any], Any] | None = None,
) -> pd.DataFrame:
    """Normalize arrival date values in batches using AI."""
    result_df = intermediate_df.copy()

    unique_dates = result_df["ë„ì°©í¬ë§ë‚ ì§œ_ì›ë³¸"].dropna().unique().tolist()

    if not unique_dates:
        return result_df

    if debug_callback:
        debug_callback("info", f"ğŸ“Š ì¶”ì¶œëœ ìœ ë‹ˆí¬ ë‚ ì§œ: {len(unique_dates)}ê°œ")
        debug_callback("unique_dates", unique_dates[:10])

    date_mapping = {}
    batch_size = 50
    total_batches = (len(unique_dates) + batch_size - 1) // batch_size

    for batch_idx in range(total_batches):
        start_idx = batch_idx * batch_size
        end_idx = min((batch_idx + 1) * batch_size, len(unique_dates))
        batch = unique_dates[start_idx:end_idx]

        if debug_callback:
            debug_callback("batch_start", f"ë°°ì¹˜ {batch_idx + 1}/{total_batches} - {len(batch)}ê°œ ë‚ ì§œ ì²˜ë¦¬ ì¤‘...")

        batch_mapping = normalize_dates_batch_with_ai(api_key, batch)

        if debug_callback:
            debug_callback("batch_result", {"batch_idx": batch_idx + 1, "mapping": batch_mapping})

        date_mapping.update(batch_mapping)

        if progress_callback:
            progress_callback(batch_idx + 1, total_batches)

    result_df["ë„ì°©í¬ë§ë‚ ì§œ_ì •ê·œí™”"] = result_df["ë„ì°©í¬ë§ë‚ ì§œ_ì›ë³¸"].map(date_mapping).fillna("")

    return result_df


def generate_cj_orders_by_date(intermediate_df: pd.DataFrame, defaults: dict[str, str]) -> dict:
    """Create CJ order files grouped by normalized date."""
    grouped = intermediate_df.groupby("ë„ì°©í¬ë§ë‚ ì§œ_ì •ê·œí™”")

    results = {}

    for date, group in grouped:
        qty = pd.to_numeric(group["ìˆ˜ëŸ‰"], errors="coerce").fillna(0).astype(int)
        item_name = group["ë³´ë‚´ì‹œëŠ”ë¶„"].fillna("OOO").astype(str) + "ë“œë¦¼ " + group["ì˜µì…˜ê´€ë¦¬ì½”ë“œ"].fillna("").astype(str)

        cj_df = pd.DataFrame(
            {
                "ë³´ë‚´ëŠ”ë¶„ì„±ëª…": defaults["name"],
                "ë³´ë‚´ëŠ”ë¶„ì „í™”ë²ˆí˜¸": defaults["phone"],
                "ë³´ë‚´ëŠ”ë¶„ì£¼ì†Œ(ì „ì²´,ë¶„í• )": defaults["address"],
                "ìš´ì„êµ¬ë¶„": "ì‹ ìš©",
                "ë°•ìŠ¤íƒ€ì…": "ê·¹ì†Œ",
                "ê¸°ë³¸ìš´ì„": qty * 2200,
                "ê³ ê°ì£¼ë¬¸ë²ˆí˜¸": group["ìƒí’ˆì£¼ë¬¸ë²ˆí˜¸"],
                "í’ˆëª©ëª…": item_name,
                "ìˆ˜ëŸ‰": qty,
                "ìˆ˜ì·¨ì¸ì´ë¦„": group["ìˆ˜ì·¨ì¸ëª…"],
                "ìˆ˜ì·¨ì¸ì „í™”ë²ˆí˜¸": group["ìˆ˜ì·¨ì¸ì—°ë½ì²˜1"],
                "ìˆ˜ì·¨ì¸ ì£¼ì†Œ": group["í†µí•©ë°°ì†¡ì§€"],
                "ë°°ì†¡ë©”ì„¸ì§€": group["ë°°ì†¡ë©”ì„¸ì§€"],
            }
        )

        buf = io.BytesIO()
        cj_df.to_excel(buf, index=False)
        buf.seek(0)

        results[date] = {"df": cj_df, "data": buf.getvalue(), "count": len(cj_df)}

    return results


def get_naver_bulk_columns() -> list[str]:
    """Column order for Naver bulk upload."""
    from pathlib import Path

    example_path = Path("output/example/naver/ë„¤ì´ë²„ ëŒ€ëŸ‰ë“±ë¡.xlsx")
    fallback = ["ìƒí’ˆì£¼ë¬¸ë²ˆí˜¸", "ë°°ì†¡ë°©ë²•", "íƒë°°ì‚¬", "ì†¡ì¥ë²ˆí˜¸"]
    if example_path.exists():
        try:
            cols = list(pd.read_excel(example_path, nrows=0).columns)
            if cols:
                return cols
        except Exception:
            pass
    return fallback


def build_naver_bulk(raw_df: pd.DataFrame, cj_df: pd.DataFrame) -> pd.DataFrame:
    """Merge Naver raw data with CJ receipt details to create bulk upload file."""
    raw_df = clean_columns(raw_df).copy()
    cj_df = clean_columns(cj_df).copy()

    # Normalize order numbers for matching
    raw_df["__key"] = raw_df["ìƒí’ˆì£¼ë¬¸ë²ˆí˜¸"].apply(_normalize_order)
    key_col = "ê³ ê°ì£¼ë¬¸ë²ˆí˜¸" if "ê³ ê°ì£¼ë¬¸ë²ˆí˜¸" in cj_df.columns else "ì£¼ë¬¸ë²ˆí˜¸"
    cj_df["__key"] = cj_df[key_col].apply(_normalize_order)

    # Debug: Print sample keys for debugging
    print("\n[DEBUG] ë„¤ì´ë²„ ëŒ€ëŸ‰ë“±ë¡ ë§¤ì¹­ ë””ë²„ê¹…:")
    print(f"- ë¡œìš°ë°ì´í„° ì´ {len(raw_df)}ê±´")
    print(f"- CJ íŒŒì¼ ì´ {len(cj_df)}ê±´")
    print(f"- CJ íŒŒì¼ì—ì„œ ì‚¬ìš©í•œ í‚¤ ì»¬ëŸ¼: {key_col}")
    print("\në¡œìš°ë°ì´í„° ìƒí’ˆì£¼ë¬¸ë²ˆí˜¸ ìƒ˜í”Œ (ì •ê·œí™” ì „ -> í›„):")
    for i in range(min(5, len(raw_df))):
        original = raw_df.iloc[i]["ìƒí’ˆì£¼ë¬¸ë²ˆí˜¸"]
        normalized = raw_df.iloc[i]["__key"]
        print(f"  {i+1}. '{original}' (íƒ€ì…: {type(original).__name__}) -> '{normalized}'")
    print("\nCJ íŒŒì¼ ê³ ê°ì£¼ë¬¸ë²ˆí˜¸ ìƒ˜í”Œ (ì •ê·œí™” ì „ -> í›„):")
    for i in range(min(5, len(cj_df))):
        original = cj_df.iloc[i][key_col]
        normalized = cj_df.iloc[i]["__key"]
        print(f"  {i+1}. '{original}' (íƒ€ì…: {type(original).__name__}) -> '{normalized}'")

    # Merge
    merged = raw_df.merge(
        cj_df[["__key", "ìš´ì†¡ì¥ë²ˆí˜¸"]],
        on="__key",
        how="left",
        suffixes=("", "_cj"),
    )

    # Debug: Check match results
    matched_count = merged["ìš´ì†¡ì¥ë²ˆí˜¸"].notna().sum() if "ìš´ì†¡ì¥ë²ˆí˜¸" in merged.columns else 0
    if "ìš´ì†¡ì¥ë²ˆí˜¸_cj" in merged.columns:
        matched_count = merged["ìš´ì†¡ì¥ë²ˆí˜¸_cj"].notna().sum()

    print(f"\në§¤ì¹­ ê²°ê³¼: {matched_count}/{len(merged)}ê±´ ë§¤ì¹­ë¨")

    # Show unmatched items
    if matched_count < len(merged):
        unmatched_mask = merged["ìš´ì†¡ì¥ë²ˆí˜¸_cj"].isna() if "ìš´ì†¡ì¥ë²ˆí˜¸_cj" in merged.columns else merged["ìš´ì†¡ì¥ë²ˆí˜¸"].isna()
        unmatched = merged[unmatched_mask]["__key"].unique()
        print(f"\në§¤ì¹­ ì•ˆ ëœ ì£¼ë¬¸ë²ˆí˜¸ ({len(unmatched)}ê°œ):")
        for i, key in enumerate(unmatched[:10]):
            print(f"  {i+1}. '{key}'")
        if len(unmatched) > 10:
            print(f"  ... ì™¸ {len(unmatched) - 10}ê°œ")

        # Check if these keys exist in CJ file
        cj_keys = set(cj_df["__key"].unique())
        print("\nCJ íŒŒì¼ì— ìˆëŠ” í‚¤ ìƒ˜í”Œ (ìµœëŒ€ 10ê°œ):")
        for i, key in enumerate(list(cj_keys)[:10]):
            print(f"  {i+1}. '{key}'")

    if "ìš´ì†¡ì¥ë²ˆí˜¸_cj" in merged:
        merged["__ì†¡ì¥"] = merged["ìš´ì†¡ì¥ë²ˆí˜¸_cj"].fillna(merged.get("ì†¡ì¥ë²ˆí˜¸"))
    else:
        merged["__ì†¡ì¥"] = merged.get("ì†¡ì¥ë²ˆí˜¸")
    merged["__ì†¡ì¥"] = merged["__ì†¡ì¥"].apply(_normalize_order)

    def pick(col, default=""):
        return merged[col] if col in merged.columns else default

    output_cols = get_naver_bulk_columns()
    data = {
        "ìƒí’ˆì£¼ë¬¸ë²ˆí˜¸": merged["__key"],
        "ë°°ì†¡ë°©ë²•": pick("ë°°ì†¡ë°©ë²•", "íƒë°°"),
        "íƒë°°ì‚¬": pick("íƒë°°ì‚¬", "CJëŒ€í•œí†µìš´"),
        "ì†¡ì¥ë²ˆí˜¸": merged["__ì†¡ì¥"],
    }

    output = pd.DataFrame(data)
    output = output[output_cols]
    return output


def build_naver_cj(df: pd.DataFrame, defaults: dict[str, str]) -> pd.DataFrame:
    """Transform Naver raw data into CJ order format."""
    required_cols = [
        "ìˆ˜ì·¨ì¸ëª…",
        "ìˆ˜ì·¨ì¸ì—°ë½ì²˜1",
        "í†µí•©ë°°ì†¡ì§€",
        "ë°°ì†¡ë©”ì„¸ì§€",
        "ìˆ˜ëŸ‰",
        "ì˜µì…˜ê´€ë¦¬ì½”ë“œ",
        "ìƒí’ˆì£¼ë¬¸ë²ˆí˜¸",
    ]
    missing = [c for c in required_cols if c not in df.columns]
    if missing:
        raise ValueError(f"ëˆ„ë½ëœ í•„ìˆ˜ ì»¬ëŸ¼: {', '.join(missing)}")

    qty = pd.to_numeric(df["ìˆ˜ëŸ‰"], errors="coerce").fillna(0).astype(int)
    item_name = "OOOë“œë¦¼ " + df["ì˜µì…˜ê´€ë¦¬ì½”ë“œ"].fillna("").astype(str)
    order_no = df["ìƒí’ˆì£¼ë¬¸ë²ˆí˜¸"].apply(_normalize_order)

    output = pd.DataFrame(
        {
            "ë³´ë‚´ëŠ”ë¶„ì„±ëª…": defaults["name"],
            "ë³´ë‚´ëŠ”ë¶„ì „í™”ë²ˆí˜¸": defaults["phone"],
            "ë³´ë‚´ëŠ”ë¶„ì£¼ì†Œ(ì „ì²´,ë¶„í• )": defaults["address"],
            "ìš´ì„êµ¬ë¶„": "ì‹ ìš©",
            "ë°•ìŠ¤íƒ€ì…": "ê·¹ì†Œ",
            "ê¸°ë³¸ìš´ì„": qty * 2200,
            "ê³ ê°ì£¼ë¬¸ë²ˆí˜¸": order_no,
            "í’ˆëª©ëª…": item_name,
            "ìˆ˜ëŸ‰": qty,
            "ìˆ˜ì·¨ì¸ì´ë¦„": df["ìˆ˜ì·¨ì¸ëª…"],
            "ìˆ˜ì·¨ì¸ì „í™”ë²ˆí˜¸": df["ìˆ˜ì·¨ì¸ì—°ë½ì²˜1"],
            "ìˆ˜ì·¨ì¸ ì£¼ì†Œ": df["í†µí•©ë°°ì†¡ì§€"],
            "ë°°ì†¡ë©”ì„¸ì§€": df["ë°°ì†¡ë©”ì„¸ì§€"],
        }
    )
    return output
