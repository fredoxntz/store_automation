import io
import json
import re
from typing import Any, Callable

import pandas as pd


def clean_columns(df: pd.DataFrame) -> pd.DataFrame:
    """Strip whitespace from column names."""
    return df.rename(columns=lambda c: str(c).strip())


def _normalize_order(value):
    """Normalize order number for matching - remove all spaces and convert to string."""
    if pd.isna(value):
        return ""
    if isinstance(value, float):
        return str(int(value))
    if isinstance(value, int):
        return str(value)
    s = str(value).strip()
    # Remove all whitespace characters (spaces, tabs, newlines, etc.)
    s = "".join(s.split())
    if s.endswith(".0") and s.replace(".", "", 1).isdigit():
        try:
            return str(int(float(s)))
        except Exception:
            return s
    return s


def parse_naver_option(option_str: str) -> dict:
    """Parse ì˜µì…˜ì •ë³´ field into structured values."""
    result = {
        "ë³´ë‚´ì‹œëŠ”ë¶„": "",
        "ë„ì°©í¬ë§ë‚ ì§œ_ì›ë³¸": "",
        "ê³¼ì¼ì„ ë¬¼ì˜µì…˜": "",
        "í¬ë¦¬ìŠ¤íƒˆë³´ìê¸°": "",
    }

    if pd.isna(option_str):
        return result

    parts = str(option_str).split(" / ")

    for part in parts:
        if ":" in part:
            key, value = part.split(":", 1)
            key = key.strip()
            value = value.strip()

            if "ë³´ë‚´ì‹œëŠ” ë¶„" in key:
                result["ë³´ë‚´ì‹œëŠ”ë¶„"] = value
            elif "ë„ì°© í¬ë§ ë‚ ì§œ" in key or "ë„ì°©í¬ë§ë‚ ì§œ" in key:
                result["ë„ì°©í¬ë§ë‚ ì§œ_ì›ë³¸"] = value
            elif "ê³¼ì¼ ì„ ë¬¼ ì˜µì…˜" in key or "ê³¼ì¼ì„ ë¬¼ì˜µì…˜" in key:
                result["ê³¼ì¼ì„ ë¬¼ì˜µì…˜"] = value
            elif "í¬ë¦¬ìŠ¤íƒˆ ë³´ìê¸°" in key:
                result["í¬ë¦¬ìŠ¤íƒˆë³´ìê¸°"] = value

    return result


def normalize_dates_batch_with_ai(api_key: str, date_list: list) -> dict:
    """Use OpenAI Responses API to normalize a batch of date strings."""
    try:
        from openai import OpenAI

        client = OpenAI(api_key=api_key)

        dates_json = json.dumps(date_list, ensure_ascii=False)

        prompt = f"""
ë‹¤ìŒ JSON ë°°ì—´ì˜ ê° ë‚ ì§œ í…ìŠ¤íŠ¸ë¥¼ MM/DD í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•´ì£¼ì„¸ìš”.
ë‚ ì§œ ì •ë³´ê°€ ë¶ˆí™•ì‹¤í•˜ë‹¤ê³  íŒë‹¨ë ë•ŒëŠ” ë¬¸ìì—´ ê·¸ëŒ€ë¡œ ë°˜í™˜í•´ì£¼ì„¸ìš”.
9ì›” 30ì¼ ë˜ëŠ” 10ì›” 1ì¼ ì´ëŸ° ë‚ ì§œëŠ” ë¬¸ìì—´ ê·¸ëŒ€ë¡œ ë°˜í™˜í•˜ì‹œì˜¤.
10ì›” 8ì¼ ìˆ˜ìš”ì¼ì²˜ëŸ¼ ìš”ì¼ì •ë³´ê°€ ìˆëŠ” ê²½ìš° 10/8ì²˜ëŸ¼ ìš”ì¼ ì •ë³´ë¥¼ ì œê±°í•˜ê³  ë‚ ì§œë§Œ ë‚¨ê¸°ì‹œì˜¤.
2025-09-30 ë°ì´í„° íƒ€ì…ë„ 9/30 ì´ëŸ°ì‹ìœ¼ë¡œ ë³€ê²½í•˜ì‹œì˜¤.
26ë…„ 2ì›” 30ì¼ ì´ëŸ°ì‹ìœ¼ë¡œ ì˜¤ëŠ” ë°ì´í„° íƒ€ì…ë„ 2/30 ì´ëŸ°ì‹ìœ¼ë¡œ ë³€ê²½í•´ì•¼í•´.
10ì›” 2ì¼ ëŠ” 10/02 ì´ëŸ°ì‹ìœ¼ë¡œ.
ëª…í™•í•œ ë‚ ì§œê°€ ì•„ë‹Œê²½ìš°ëŠ” ë³€í™˜í•˜ì§€ ë§ê³  ê·¸ëŒ€ë¡œ ë‹¤ì‹œ ë³€í™˜ê²°ê³¼ì— ë„£ì–´ì¤˜.

ê²°êµ­ ë‚ ì§œ ë°ì´í„°ë¥¼ ë³€í™˜í• ë•Œ ë‚´ê°€ ì›í•˜ëŠ” ìµœì¢… ë‚ ì§œ ë³€í™˜ í˜•íƒœëŠ” MM/DDì•¼. ê¼­ ì´ë ‡ê²Œ ë³€í™˜í•´ì„œ ê²°ê³¼ë¥¼ ë§Œë“¤ì–´ì£¼ê¸¸ ì›í•´.

ì…ë ¥: {dates_json}

ì¶œë ¥ì€ ë°˜ë“œì‹œ "ì›ë³¸": "ë³€í™˜ê²°ê³¼" í˜•íƒœì˜ JSON ê°ì²´ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”. ì„¤ëª…ì€ í•˜ì§€ ë§ˆì„¸ìš”.
ì˜ˆì‹œ: {{"9ì›”30ì¼": "09/30", "10/1": "10/01", "ìµœëŒ€í•œ ë¹¨ë¦¬": "ìµœëŒ€í•œ ë¹¨ë¦¬", "10ì›” 2ì¼": "10/2", 10ì›” 8ì¼ ìˆ˜ìš”ì¼ : "10/8}}
"""

        response = client.responses.create(
            model="gpt-4.1-nano-2025-04-14",
            input=prompt,
            max_output_tokens=1000,
        )

        result_text = (response.output_text or "").strip()

        if not result_text:
            return {date: "ì˜¤ë¥˜: ë¹ˆ ì‘ë‹µ" for date in date_list}

        if result_text.startswith("```"):
            parts = result_text.split("```")
            if len(parts) >= 2:
                result_text = parts[1]
            result_text = result_text.replace("json", "").strip()

        json_match = re.search(r"\{.*\}", result_text, re.DOTALL)
        if json_match:
            result_text = json_match.group(0).strip()
        else:
            return {date: "ì˜¤ë¥˜: JSON ì¶œë ¥ ì•„ë‹˜" for date in date_list}

        try:
            return json.loads(result_text)
        except Exception:
            return {date: f"ì˜¤ë¥˜: JSON íŒŒì‹± ì‹¤íŒ¨: {result_text}" for date in date_list}

    except Exception as e:
        return {date: f"ì˜¤ë¥˜: {str(e)}" for date in date_list}


def create_naver_intermediate_table(df: pd.DataFrame, api_key: str | None = None) -> pd.DataFrame:
    """Build intermediate table from raw Naver export."""
    parsed_options = df["ì˜µì…˜ì •ë³´"].apply(parse_naver_option)
    parsed_df = pd.DataFrame(parsed_options.tolist())

    intermediate = pd.DataFrame(
        {
            "ìƒí’ˆì£¼ë¬¸ë²ˆí˜¸": df["ìƒí’ˆì£¼ë¬¸ë²ˆí˜¸"],
            "ìˆ˜ì·¨ì¸ëª…": df["ìˆ˜ì·¨ì¸ëª…"],
            "ìˆ˜ì·¨ì¸ì—°ë½ì²˜1": df["ìˆ˜ì·¨ì¸ì—°ë½ì²˜1"],
            "í†µí•©ë°°ì†¡ì§€": df["í†µí•©ë°°ì†¡ì§€"],
            "ë°°ì†¡ë©”ì„¸ì§€": df["ë°°ì†¡ë©”ì„¸ì§€"],
            "ìˆ˜ëŸ‰": df["ìˆ˜ëŸ‰"],
            "ì˜µì…˜ê´€ë¦¬ì½”ë“œ": df["ì˜µì…˜ê´€ë¦¬ì½”ë“œ"],
            "ë³´ë‚´ì‹œëŠ”ë¶„": parsed_df["ë³´ë‚´ì‹œëŠ”ë¶„"],
            "ë„ì°©í¬ë§ë‚ ì§œ_ì›ë³¸": parsed_df["ë„ì°©í¬ë§ë‚ ì§œ_ì›ë³¸"],
            "ë„ì°©í¬ë§ë‚ ì§œ_ì •ê·œí™”": "",
            "ê³¼ì¼ì„ ë¬¼ì˜µì…˜": parsed_df["ê³¼ì¼ì„ ë¬¼ì˜µì…˜"],
        }
    )

    return intermediate


def normalize_dates_batch(
    intermediate_df: pd.DataFrame,
    api_key: str,
    progress_callback: Callable[[int, int], Any] | None = None,
    debug_callback: Callable[[str, Any], Any] | None = None,
) -> pd.DataFrame:
    """Normalize arrival date values in batches using AI."""
    result_df = intermediate_df.copy()

    unique_dates = result_df["ë„ì°©í¬ë§ë‚ ì§œ_ì›ë³¸"].dropna().unique().tolist()

    if not unique_dates:
        return result_df

    if debug_callback:
        debug_callback("info", f"ğŸ“Š ì¶”ì¶œëœ ìœ ë‹ˆí¬ ë‚ ì§œ: {len(unique_dates)}ê°œ")
        debug_callback("unique_dates", unique_dates[:10])

    date_mapping = {}
    batch_size = 50
    total_batches = (len(unique_dates) + batch_size - 1) // batch_size

    for batch_idx in range(total_batches):
        start_idx = batch_idx * batch_size
        end_idx = min((batch_idx + 1) * batch_size, len(unique_dates))
        batch = unique_dates[start_idx:end_idx]

        if debug_callback:
            debug_callback("batch_start", f"ë°°ì¹˜ {batch_idx + 1}/{total_batches} - {len(batch)}ê°œ ë‚ ì§œ ì²˜ë¦¬ ì¤‘...")

        batch_mapping = normalize_dates_batch_with_ai(api_key, batch)

        if debug_callback:
            debug_callback("batch_result", {"batch_idx": batch_idx + 1, "mapping": batch_mapping})

        date_mapping.update(batch_mapping)

        if progress_callback:
            progress_callback(batch_idx + 1, total_batches)

    result_df["ë„ì°©í¬ë§ë‚ ì§œ_ì •ê·œí™”"] = result_df["ë„ì°©í¬ë§ë‚ ì§œ_ì›ë³¸"].map(date_mapping).fillna("")

    return result_df


def _is_valid_date(date_str: str) -> bool:
    """Check if a string is a valid MM/DD format date."""
    if pd.isna(date_str) or not date_str:
        return False
    date_str = str(date_str).strip()
    # MM/DD ë˜ëŠ” M/D ë˜ëŠ” MM/D ë˜ëŠ” M/DD í˜•ì‹ì¸ì§€ í™•ì¸
    pattern = r'^\d{1,2}/\d{1,2}$'
    return bool(re.match(pattern, date_str))


def _create_sort_key(row):
    """Create a sort key for ordering: invalid dates first, then by date, then by option code."""
    date_str = str(row["ë„ì°©í¬ë§ë‚ ì§œ_ì •ê·œí™”"]).strip()
    option_code = str(row["ì˜µì…˜ê´€ë¦¬ì½”ë“œ"]).strip()

    # ë‚ ì§œê°€ ìœ íš¨í•˜ì§€ ì•Šìœ¼ë©´ (0, date_str, option_code)ë¡œ ì •ë ¬ -> ê°€ì¥ ìœ„ë¡œ
    if not _is_valid_date(date_str):
        return (0, date_str, option_code)

    # ë‚ ì§œê°€ ìœ íš¨í•˜ë©´ (1, ì›”, ì¼, option_code)ë¡œ ì •ë ¬
    try:
        parts = date_str.split('/')
        month = int(parts[0])
        day = int(parts[1])
        return (1, month, day, option_code)
    except:
        # íŒŒì‹± ì‹¤íŒ¨ì‹œ ë‚ ì§œ ë¶ˆë¶„ëª…ìœ¼ë¡œ ì²˜ë¦¬
        return (0, date_str, option_code)


def generate_cj_orders_by_date(intermediate_df: pd.DataFrame, defaults: dict[str, str]) -> dict:
    """Create a single CJ order file with all dates, sorted by date validity, then date, then option code."""
    import datetime as dt

    # í’ˆëª©ëª…ì— ë‚ ì§œ ì¶”ê°€: ë³´ë‚´ì‹œëŠ”ë¶„ + "ë“œë¦¼ " + ì˜µì…˜ê´€ë¦¬ì½”ë“œ + " " + ë‚ ì§œ
    qty = pd.to_numeric(intermediate_df["ìˆ˜ëŸ‰"], errors="coerce").fillna(0).astype(int)

    item_name = (
        intermediate_df["ë³´ë‚´ì‹œëŠ”ë¶„"].fillna("OOO").astype(str)
        + "ë“œë¦¼ "
        + intermediate_df["ì˜µì…˜ê´€ë¦¬ì½”ë“œ"].fillna("").astype(str)
        + " "
        + intermediate_df["ë„ì°©í¬ë§ë‚ ì§œ_ì •ê·œí™”"].fillna("").astype(str)
    )

    cj_df = pd.DataFrame(
        {
            "ë³´ë‚´ëŠ”ë¶„ì„±ëª…": defaults["name"],
            "ë³´ë‚´ëŠ”ë¶„ì „í™”ë²ˆí˜¸": defaults["phone"],
            "ë³´ë‚´ëŠ”ë¶„ì£¼ì†Œ(ì „ì²´,ë¶„í• )": defaults["address"],
            "ìš´ì„êµ¬ë¶„": "ì‹ ìš©",
            "ë°•ìŠ¤íƒ€ì…": "ê·¹ì†Œ",
            "ê¸°ë³¸ìš´ì„": qty * 2200,
            "ê³ ê°ì£¼ë¬¸ë²ˆí˜¸": intermediate_df["ìƒí’ˆì£¼ë¬¸ë²ˆí˜¸"],
            "í’ˆëª©ëª…": item_name,
            "ìˆ˜ëŸ‰": qty,
            "ìˆ˜ì·¨ì¸ì´ë¦„": intermediate_df["ìˆ˜ì·¨ì¸ëª…"],
            "ìˆ˜ì·¨ì¸ì „í™”ë²ˆí˜¸": intermediate_df["ìˆ˜ì·¨ì¸ì—°ë½ì²˜1"],
            "ìˆ˜ì·¨ì¸ ì£¼ì†Œ": intermediate_df["í†µí•©ë°°ì†¡ì§€"],
            "ë°°ì†¡ë©”ì„¸ì§€": intermediate_df["ë°°ì†¡ë©”ì„¸ì§€"],
            "ë„ì°©í¬ë§ë‚ ì§œ_ì •ê·œí™”": intermediate_df["ë„ì°©í¬ë§ë‚ ì§œ_ì •ê·œí™”"],  # ì •ë ¬ìš©
            "ì˜µì…˜ê´€ë¦¬ì½”ë“œ": intermediate_df["ì˜µì…˜ê´€ë¦¬ì½”ë“œ"],  # ì •ë ¬ìš©
        }
    )

    # ì •ë ¬: 1) ë‚ ì§œ ë¶ˆë¶„ëª…í•œ ê²ƒ ìœ„ë¡œ, 2) ë‚ ì§œìˆœ, 3) ì˜µì…˜ê´€ë¦¬ì½”ë“œìˆœ
    cj_df['__sort_key'] = cj_df.apply(_create_sort_key, axis=1)
    cj_df = cj_df.sort_values('__sort_key').reset_index(drop=True)

    # ì •ë ¬ì— ì‚¬ìš©í•œ ì„ì‹œ ì»¬ëŸ¼ ì œê±°
    cj_df = cj_df.drop(columns=['__sort_key', 'ë„ì°©í¬ë§ë‚ ì§œ_ì •ê·œí™”', 'ì˜µì…˜ê´€ë¦¬ì½”ë“œ'])

    buf = io.BytesIO()
    cj_df.to_excel(buf, index=False)
    buf.seek(0)

    # íŒŒì¼ëª…ì— ì˜¤ëŠ˜ ë‚ ì§œ í¬í•¨
    today = dt.datetime.now().strftime("%y%m%d")
    filename = f"ë„¤ì´ë²„_CJë°œì£¼ì„œ_{today}.xlsx"

    results = {
        "single": {
            "df": cj_df,
            "data": buf.getvalue(),
            "count": len(cj_df),
            "filename": filename
        }
    }

    return results


def get_naver_bulk_columns() -> list[str]:
    """Column order for Naver bulk upload."""
    from pathlib import Path

    example_path = Path("output/example/naver/ë„¤ì´ë²„ ëŒ€ëŸ‰ë“±ë¡.xlsx")
    fallback = ["ìƒí’ˆì£¼ë¬¸ë²ˆí˜¸", "ë°°ì†¡ë°©ë²•", "íƒë°°ì‚¬", "ì†¡ì¥ë²ˆí˜¸"]
    if example_path.exists():
        try:
            cols = list(pd.read_excel(example_path, nrows=0).columns)
            if cols:
                return cols
        except Exception:
            pass
    return fallback


def build_naver_bulk(raw_df: pd.DataFrame, cj_df: pd.DataFrame) -> tuple[pd.DataFrame, dict]:
    """Merge Naver raw data with CJ receipt details to create bulk upload file.

    Returns:
        tuple: (output_df, debug_info)
    """
    raw_df = clean_columns(raw_df).copy()
    cj_df = clean_columns(cj_df).copy()

    # Normalize order numbers for matching
    raw_df["__key"] = raw_df["ìƒí’ˆì£¼ë¬¸ë²ˆí˜¸"].apply(_normalize_order)
    key_col = "ê³ ê°ì£¼ë¬¸ë²ˆí˜¸" if "ê³ ê°ì£¼ë¬¸ë²ˆí˜¸" in cj_df.columns else "ì£¼ë¬¸ë²ˆí˜¸"
    cj_df["__key"] = cj_df[key_col].apply(_normalize_order)

    # Collect debug info
    debug_info = {
        "raw_count": len(raw_df),
        "cj_count": len(cj_df),
        "key_col": key_col,
        "raw_samples": [],
        "cj_samples": [],
    }

    for i in range(min(5, len(raw_df))):
        original = raw_df.iloc[i]["ìƒí’ˆì£¼ë¬¸ë²ˆí˜¸"]
        normalized = raw_df.iloc[i]["__key"]
        debug_info["raw_samples"].append({
            "original": str(original),
            "type": type(original).__name__,
            "normalized": normalized
        })

    for i in range(min(5, len(cj_df))):
        original = cj_df.iloc[i][key_col]
        normalized = cj_df.iloc[i]["__key"]
        invoice = cj_df.iloc[i].get("ìš´ì†¡ì¥ë²ˆí˜¸", "")
        debug_info["cj_samples"].append({
            "original": str(original),
            "type": type(original).__name__,
            "normalized": normalized,
            "invoice": str(invoice) if pd.notna(invoice) else ""
        })

    # CJ íŒŒì¼ì— ìš´ì†¡ì¥ë²ˆí˜¸ ì»¬ëŸ¼ì´ ìˆëŠ”ì§€ í™•ì¸
    debug_info["has_invoice_col"] = "ìš´ì†¡ì¥ë²ˆí˜¸" in cj_df.columns

    # Merge - ìš´ì†¡ì¥ë²ˆí˜¸ ì»¬ëŸ¼ì´ ìˆìœ¼ë©´ í¬í•¨, ì—†ìœ¼ë©´ __keyë§Œ ì‚¬ìš©
    merge_cols = ["__key"]
    if "ìš´ì†¡ì¥ë²ˆí˜¸" in cj_df.columns:
        merge_cols.append("ìš´ì†¡ì¥ë²ˆí˜¸")

    merged = raw_df.merge(
        cj_df[merge_cols],
        on="__key",
        how="left",
        suffixes=("", "_cj"),
    )

    # Debug: Check match results
    matched_count = merged["ìš´ì†¡ì¥ë²ˆí˜¸"].notna().sum() if "ìš´ì†¡ì¥ë²ˆí˜¸" in merged.columns else 0
    if "ìš´ì†¡ì¥ë²ˆí˜¸_cj" in merged.columns:
        matched_count = merged["ìš´ì†¡ì¥ë²ˆí˜¸_cj"].notna().sum()

    debug_info["matched_count"] = matched_count
    debug_info["total_count"] = len(merged)

    # Show unmatched items
    if matched_count < len(merged):
        unmatched_mask = merged["ìš´ì†¡ì¥ë²ˆí˜¸_cj"].isna() if "ìš´ì†¡ì¥ë²ˆí˜¸_cj" in merged.columns else merged["ìš´ì†¡ì¥ë²ˆí˜¸"].isna()
        unmatched = merged[unmatched_mask]["__key"].unique().tolist()
        debug_info["unmatched"] = unmatched[:10]
        debug_info["unmatched_count"] = len(unmatched)

        # Check if these keys exist in CJ file
        cj_keys = set(cj_df["__key"].unique())
        debug_info["cj_keys_sample"] = list(cj_keys)[:10]

    # ì†¡ì¥ë²ˆí˜¸ ì²˜ë¦¬: CJ íŒŒì¼ì—ì„œ ê°€ì ¸ì˜¨ ìš´ì†¡ì¥ë²ˆí˜¸ ì‚¬ìš©
    if "ìš´ì†¡ì¥ë²ˆí˜¸_cj" in merged:
        merged["__ì†¡ì¥"] = merged["ìš´ì†¡ì¥ë²ˆí˜¸_cj"]
    elif "ìš´ì†¡ì¥ë²ˆí˜¸" in merged:
        merged["__ì†¡ì¥"] = merged["ìš´ì†¡ì¥ë²ˆí˜¸"]
    elif "ì†¡ì¥ë²ˆí˜¸" in merged:
        merged["__ì†¡ì¥"] = merged["ì†¡ì¥ë²ˆí˜¸"]
    else:
        merged["__ì†¡ì¥"] = ""

    # ì†¡ì¥ë²ˆí˜¸ ì •ê·œí™” (NaNì„ ë¹ˆ ë¬¸ìì—´ë¡œ, ìˆ«ìë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜)
    merged["__ì†¡ì¥"] = merged["__ì†¡ì¥"].apply(_normalize_order)

    def pick(col, default=""):
        """ì»¬ëŸ¼ì´ ì—†ê±°ë‚˜ ê°’ì´ ë¹„ì–´ìˆìœ¼ë©´ default ë°˜í™˜"""
        if col not in merged.columns:
            return default
        # ì»¬ëŸ¼ì€ ìˆì§€ë§Œ ëª¨ë“  ê°’ì´ ë¹„ì–´ìˆìœ¼ë©´ default ë°˜í™˜
        col_data = merged[col].fillna("")
        if col_data.astype(str).str.strip().eq("").all():
            return default
        return merged[col]

    output_cols = get_naver_bulk_columns()
    data = {
        "ìƒí’ˆì£¼ë¬¸ë²ˆí˜¸": merged["__key"],
        "ë°°ì†¡ë°©ë²•": pick("ë°°ì†¡ë°©ë²•", "íƒë°°"),
        "íƒë°°ì‚¬": "CJ ëŒ€í•œí†µìš´",  # í•­ìƒ CJ ëŒ€í•œí†µìš´ìœ¼ë¡œ ì„¤ì •
        "ì†¡ì¥ë²ˆí˜¸": merged["__ì†¡ì¥"],
    }

    output = pd.DataFrame(data)
    output = output[output_cols]

    # ìƒí’ˆì£¼ë¬¸ë²ˆí˜¸ ì¤‘ë³µ ì œê±° (ì²« ë²ˆì§¸ í–‰ë§Œ ìœ ì§€)
    output = output.drop_duplicates(subset=['ìƒí’ˆì£¼ë¬¸ë²ˆí˜¸'], keep='first')

    return output, debug_info


def build_naver_cj(df: pd.DataFrame, defaults: dict[str, str]) -> pd.DataFrame:
    """Transform Naver raw data into CJ order format."""
    required_cols = [
        "ìˆ˜ì·¨ì¸ëª…",
        "ìˆ˜ì·¨ì¸ì—°ë½ì²˜1",
        "í†µí•©ë°°ì†¡ì§€",
        "ë°°ì†¡ë©”ì„¸ì§€",
        "ìˆ˜ëŸ‰",
        "ì˜µì…˜ê´€ë¦¬ì½”ë“œ",
        "ìƒí’ˆì£¼ë¬¸ë²ˆí˜¸",
    ]
    missing = [c for c in required_cols if c not in df.columns]
    if missing:
        raise ValueError(f"ëˆ„ë½ëœ í•„ìˆ˜ ì»¬ëŸ¼: {', '.join(missing)}")

    qty = pd.to_numeric(df["ìˆ˜ëŸ‰"], errors="coerce").fillna(0).astype(int)
    item_name = "OOOë“œë¦¼ " + df["ì˜µì…˜ê´€ë¦¬ì½”ë“œ"].fillna("").astype(str)
    order_no = df["ìƒí’ˆì£¼ë¬¸ë²ˆí˜¸"].apply(_normalize_order)

    output = pd.DataFrame(
        {
            "ë³´ë‚´ëŠ”ë¶„ì„±ëª…": defaults["name"],
            "ë³´ë‚´ëŠ”ë¶„ì „í™”ë²ˆí˜¸": defaults["phone"],
            "ë³´ë‚´ëŠ”ë¶„ì£¼ì†Œ(ì „ì²´,ë¶„í• )": defaults["address"],
            "ìš´ì„êµ¬ë¶„": "ì‹ ìš©",
            "ë°•ìŠ¤íƒ€ì…": "ê·¹ì†Œ",
            "ê¸°ë³¸ìš´ì„": qty * 2200,
            "ê³ ê°ì£¼ë¬¸ë²ˆí˜¸": order_no,
            "í’ˆëª©ëª…": item_name,
            "ìˆ˜ëŸ‰": qty,
            "ìˆ˜ì·¨ì¸ì´ë¦„": df["ìˆ˜ì·¨ì¸ëª…"],
            "ìˆ˜ì·¨ì¸ì „í™”ë²ˆí˜¸": df["ìˆ˜ì·¨ì¸ì—°ë½ì²˜1"],
            "ìˆ˜ì·¨ì¸ ì£¼ì†Œ": df["í†µí•©ë°°ì†¡ì§€"],
            "ë°°ì†¡ë©”ì„¸ì§€": df["ë°°ì†¡ë©”ì„¸ì§€"],
        }
    )
    return output
